{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94cc029b-0d55-4891-afb4-05961f66d729",
   "metadata": {},
   "source": [
    "# Problem 1\n",
    "\n",
    "Setting up the enrivonment and loading the data. No need to change this part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce6084d-0ac9-46e7-b74d-367ea80b5670",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Load tensors from the file\n",
    "loaded_tensors = torch.load('exercise_2_problem_1_data.pth')\n",
    "X_tensor = loaded_tensors['X_tensor']\n",
    "Y_tensor = loaded_tensors['Y_tensor']\n",
    "dataset = TensorDataset(X_tensor, Y_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879c23b7",
   "metadata": {},
   "source": [
    "## a) Model definition\n",
    "\n",
    "Finalize the model definition as instructed in the exercise sheet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19f2fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressionModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            # TODO\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e71365f",
   "metadata": {},
   "source": [
    "## b) Write optimization loop\n",
    "\n",
    "You can use any optimizer you want, but remember to set all the hyperparameters it requires, including the batch size and number of iterations etc that are defined outside the optimizer function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e294969d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper for retrieving minibatches of desired size\n",
    "batch_size = # You choose this!\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "model = RegressionModel(100, 2)\n",
    "criterion = nn.MSELoss()\n",
    "lr = # TODO\n",
    "optimizer = # TODO\n",
    "\n",
    "num_epochs = # TODO\n",
    "printing_interval = num_epochs // 10 # Avoid printing hundreds or thousands of losses, to keep the notebook cleaner\n",
    "losses_ = []\n",
    "\n",
    "model.train()\n",
    "# TODO: Define loop over epochs \n",
    "    running_loss = 0.0\n",
    "\n",
    "    # TODO: Define loop over batches\n",
    "    \n",
    "        # 1) TODO: Zero the gradients\n",
    "        # 2) TODO: Forward pass\n",
    "        # 3) TODO: Compute loss (MSE)\n",
    "        # 4) TODO: Backprop\n",
    "        # 5) TODO: Update parameters\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    avg_loss = running_loss / len(dataloader)\n",
    "    losses_.append(avg_loss)\n",
    "\n",
    "    if(epoch % printing_interval == 1):\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] - MSE Loss: {avg_loss:.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27acb6b3",
   "metadata": {},
   "source": [
    "## d) Computing the gradient norms\n",
    "\n",
    "Practice extracting information about a trained model, by computing the gradient norms that were used to analyse SGD behavior during the lectures. No need to do anything with the gradient norms, just show that you can compute them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311a40aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch to evaluation mode -- we are no longer training!\n",
    "model.eval()\n",
    "\n",
    "grad_norms = []  # to store gradient norms for each batch\n",
    "\n",
    "# TODO: Define loop over batches\n",
    "    # Compute squared L2 norm across all model parameter gradients\n",
    "    # Hints:\n",
    "    # - You again need to first evaluate the gradient of the loss wrt to the model parameters\n",
    "    # - You can extract a list of all model parameters with model.parameters()\n",
    "    # - For each parameter p in model.parameters(), you can access the gradient with p.grad\n",
    "    grad_norms.append(norm_for_this_batch)\n",
    "\n",
    "# Convert to numpy array for statistics\n",
    "grad_norms = np.array(grad_norms)\n",
    "mean_gn = # TODO\n",
    "var_gn  = # TODO\n",
    "\n",
    "print(\"Gradient Norm stats for evaluation pass:\")\n",
    "print(f\"  Mean  : {mean_gn:.6f}\")\n",
    "print(f\"  Std   : {var_gn:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c173449-0811-48b1-a2bc-50aa9d184b60",
   "metadata": {},
   "source": [
    "## e) Reporting\n",
    "\n",
    "**Edit this cell directly to write your answers.** You should print the values within the code blocks, but also copy them here for ease of grading. You can also write any other remarks you may have in this cell.\n",
    "\n",
    "### Optimizer settings: \n",
    "\n",
    "TODO: I used ... with parameters ...\n",
    "\n",
    "### Optimization speed:\n",
    "\n",
    "TODO: I reached the loss threshold after ... epochs\n",
    "\n",
    "### The gradient norms:\n",
    "\n",
    "TODO: After training the model, the gradient norm is ... and the variance over the batches is ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fd401c-c9a6-4850-9219-29f2f19f4ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss as function of epochs, using log-scale for the y-axis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43dd42e",
   "metadata": {},
   "source": [
    "=========\n",
    "\n",
    "# Problem 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361f9014-7dc5-406f-aa27-afbbe26b8f11",
   "metadata": {},
   "source": [
    "## a) Generate training/testing data\n",
    "\n",
    "Do not change this part. The code snippet greates synthetic data by feeding random inputs through randomly initialized neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbd36f1-0ecc-49af-85e6-61631a2136e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# N samples with D inputs and O outputs\n",
    "N = 100\n",
    "D = 10\n",
    "O = 5\n",
    "\n",
    "# Random mapping from x to y, as small neural network\n",
    "class CreationModel(nn.Module):\n",
    "  def __init__(self, D, O, M):\n",
    "    super().__init__()\n",
    "    self.layers = nn.Sequential(\n",
    "      nn.Linear(D, M),\n",
    "      nn.Tanh(),\n",
    "      nn.Linear(M, O)\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    return self.layers(x)\n",
    "\n",
    "# For getting the same data\n",
    "torch.manual_seed(78798)\n",
    "\n",
    "# Training data\n",
    "x = torch.randn(N, D)\n",
    "noiselevel = 0.1\n",
    "modelGenerate = CreationModel(D,O,5)\n",
    "modelGenerate.eval()\n",
    "y = modelGenerate(x).clone().detach() + noiselevel*torch.randn(N, O)\n",
    "\n",
    "# Test data\n",
    "N_test = 10000\n",
    "x_test = torch.randn(N_test, D)\n",
    "y_test = modelGenerate(x_test).clone().detach()  + noiselevel*torch.randn(N_test, O)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ae6dff-fd03-4b3b-872b-6a3bc9d5d7f7",
   "metadata": {},
   "source": [
    "## Pre-defined model\n",
    "\n",
    "Do not change this part. The parameter $M$ controls the size/complexity of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f9fba9-6817-43ea-86fc-c2b0a2f8a7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressionModel(nn.Module):\n",
    "  def __init__(self, D, O, M):\n",
    "    super().__init__()\n",
    "    self.layers = nn.Sequential(\n",
    "      nn.Linear(D, M),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(M, M),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(M, O)\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9682148-ec80-4bf8-afc9-6e0eea3126ae",
   "metadata": {},
   "source": [
    "## b) Train model and validate the double descent principle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287db507-0292-47cd-8d83-773806614813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use full data for gradients\n",
    "B = N\n",
    "data_loader = DataLoader(TensorDataset(x,y), batch_size=B, shuffle=True)\n",
    "\n",
    "# Loop over some range of M values. With these parameters logspace generates M=2 twice, so dropping the first entry\n",
    "Mvalues = np.logspace(np.log10(2),np.log10(60),num=15,dtype='int')[1:]\n",
    "losses = np.zeros((len(Mvalues),2))\n",
    "for M_index, M in enumerate(Mvalues):\n",
    "    # TODO: Define the model\n",
    "    # TODO: Train it until convergence\n",
    "    # TODO: Evaluate both training and test error, and store them\n",
    "\n",
    "    losses[mi,0] = trainloss.item()\n",
    "    losses[mi,1] = testloss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8494fe7",
   "metadata": {},
   "source": [
    "### Plot the final training loss and the final test loss\n",
    "\n",
    "The code below is provided as an example; you may need to modify it to be compatible with your results. Make some effort to make the plot easy to read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245266e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.loglog(Mvalues, losses[:,0], 'b-')\n",
    "plt.loglog(Mvalues, losses[:,1], 'b:')\n",
    "plt.legend(['Training loss', 'Test loss'])\n",
    "ax = plt.gca(); ax.set_ylim([10**(-5),10**(0)])\n",
    "ax.set_ylabel(\"MSE\")\n",
    "ax.set_xlabel(\"Number of neurons\")\n",
    "# Customize x-axis to show more detailed labels\n",
    "from matplotlib.ticker import LogLocator, ScalarFormatter\n",
    "ax.xaxis.set_major_locator(LogLocator(base=10.0, subs=np.arange(1, 10)))\n",
    "ax.xaxis.set_major_formatter(ScalarFormatter())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7de7b2-e7ce-4141-8bb5-eb5ce0b6a525",
   "metadata": {},
   "source": [
    "### c) Having more data can hurt\n",
    "\n",
    "Show that increasing the amount of data helps when around the interpolation threshold, but that for overparameterized model it may hurt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c3f98d-ebd6-4c45-9d4a-7af66f4fb5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the larger training set, using the same process as before\n",
    "N_large = 500\n",
    "x_large = torch.randn(N_large, D)\n",
    "y_large = modelGenerate(x_large).clone().detach() + noiselevel*torch.randn(N_large, O)\n",
    "\n",
    "# TODO: Identify two values of M you need to answer the question\n",
    "\n",
    "# TODO: Repeat the training process for those options. Note that you can use the same code as before, \n",
    "# since you only changed the data and the set of M\n",
    "\n",
    "print(\"Test losses\")\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0561e620",
   "metadata": {},
   "source": [
    "## Reporting\n",
    "\n",
    "**Edit this cell directly to report the requested information.**\n",
    "\n",
    "### Interpolation threshold\n",
    "\n",
    "TODO: The interpolation threshold is around M = ...\n",
    "\n",
    "TODO: For that M, the model has in total ... parameters\n",
    "\n",
    "TODO: Explain the findings. Do you observe the expected pattern? If not, what might have been a problem?\n",
    "\n",
    "### More data can hurt\n",
    "\n",
    "TODO: Explain your choices of M values, report the test errors for the different amounts of training data, and explain the findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59b6437",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
