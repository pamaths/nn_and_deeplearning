{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e5598e8-2871-42ad-95c0-06c01af90387",
   "metadata": {},
   "source": [
    "# 1. Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a567fc-410a-4627-9e2a-0dbce23b8127",
   "metadata": {},
   "source": [
    "To install Optuna: run `pip install optuna` in console after activating your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4046888a-c3b2-4ed5-afbf-852b37af5d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, random_split, ConcatDataset, Subset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "import optuna\n",
    "import optuna.visualization as vis\n",
    "\n",
    "seed = 42\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b39ae3-23c6-4610-a84c-9d76fc3e6416",
   "metadata": {},
   "source": [
    "Load MNIST data. **DO NOT CHANGE THIS.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202b9568-81b1-4051-bfbf-207deb1fb7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mnist_loaders(batch_size, valid_ratio, seed, downsample_ratio):\n",
    "    # Define the transformation: convert to tensor and normalize\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "    \n",
    "    # Load the full training set (train=True returns the 60k training images)\n",
    "    train_dataset = torchvision.datasets.MNIST(\n",
    "        root=\"data\",\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=transform\n",
    "    )\n",
    "    \n",
    "    # Optionally downsample the training data\n",
    "    if downsample_ratio < 1.0:\n",
    "        subset_size = int(len(train_dataset) * downsample_ratio)\n",
    "        indices = list(range(subset_size))\n",
    "        train_dataset = Subset(train_dataset, indices)\n",
    "    \n",
    "    # Load the test set (train=False returns the 10k testing images)\n",
    "    test_dataset = torchvision.datasets.MNIST(\n",
    "        root=\"data\",\n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=transform\n",
    "    )\n",
    "    \n",
    "    # Split training set into training and validation set\n",
    "    train_size = int((1.0 - valid_ratio) * len(train_dataset))\n",
    "    valid_size = len(train_dataset) - train_size\n",
    "    gen = torch.Generator().manual_seed(seed)\n",
    "    train_dataset, valid_dataset = random_split(train_dataset, [train_size, valid_size], generator=gen)\n",
    "    \n",
    "    # DataLoaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader  = DataLoader(test_dataset,  batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return train_loader, valid_loader, test_loader\n",
    "\n",
    "batch_size = 128\n",
    "valid_ratio = 0.2\n",
    "downsample_ratio = 0.5\n",
    "\n",
    "# Create the DataLoaders\n",
    "train_loader, valid_loader, test_loader = get_mnist_loaders(batch_size=batch_size, \n",
    "                                                              valid_ratio=valid_ratio, \n",
    "                                                              seed=seed,\n",
    "                                                              downsample_ratio=downsample_ratio)\n",
    "\n",
    "# Total number of images in each dataset\n",
    "print(\"Total training images:\", len(train_loader.dataset))\n",
    "print(\"Total validation images:\", len(valid_loader.dataset))\n",
    "print(\"Total test images:\", len(test_loader.dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe935f6a-2b13-4f02-861e-23e1e8f49002",
   "metadata": {},
   "source": [
    "## (1a) Implement CNN\n",
    "\n",
    "Write here the network structure as described in the exercise sheet.\n",
    "\n",
    "Remember to **make the kernel size a parameter**, so that it can be controlled later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ecc5eb-df11-49b7-b3cd-5171f6ae735e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    \n",
    "    # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10fc8a2-2ccb-4c62-935e-8437206c77c0",
   "metadata": {},
   "source": [
    "#### (1b & 1c) Train&Test Functions\n",
    "\n",
    "To help focus on tuning optimizers and networks, we provided 3 pre-written functions for training and testing.\n",
    "\n",
    "By having clear functions, we can easily compare different settings and keep the process consistent.\n",
    "\n",
    "**You do not need to change anything in this part.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c47dd7-0f6b-440f-9ea9-0cdbb4c762eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function.\n",
    "def train(net, train_loader, parameters, epochs=1):\n",
    "    optimizer = torch.optim.SGD(net.parameters(), lr=parameters[\"lr\"], momentum=parameters[\"momentum\"])\n",
    "    criterion = nn.NLLLoss()\n",
    "    net.train()\n",
    "    for epoch in range(epochs):\n",
    "        for images, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    return net\n",
    "\n",
    "# Evaluation function.\n",
    "def evaluate(net, data_loader):\n",
    "    net.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loader:\n",
    "            outputs = net(images)\n",
    "            predictions = outputs.argmax(dim=1)\n",
    "            correct += (predictions == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    accuracy = correct / total\n",
    "    return accuracy\n",
    "\n",
    "# Function for final training using best params on combined train and validation sets, then testing.\n",
    "def train_test(parameters):\n",
    "    combined_dataset = ConcatDataset([train_loader.dataset, valid_loader.dataset])\n",
    "    combined_loader = DataLoader(combined_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    net = CNN(kernel_size=parameters[\"kernel_size\"])\n",
    "    net = train(net, combined_loader, parameters)\n",
    "    test_accuracy = evaluate(net, test_loader)\n",
    "    return test_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c15c5e-7586-41bf-aeeb-453aa53a9a91",
   "metadata": {},
   "source": [
    "## (1b) Bayesian optimization using Optuna\n",
    "\n",
    "Optuna requires an objective for the optimization.\n",
    "\n",
    "Our objective returns validation accuracy, and we use **direction=\"maximize\".**\n",
    "\n",
    "So, we aim to maximize the accuracy of the validation dataset.\n",
    "\n",
    "**Here, your task is to define the hyperparameter search spaces** (suggested hyperparameters).\n",
    "\n",
    "Here you can see the documentary page for search spaces:\n",
    "\n",
    "https://optuna.readthedocs.io/en/stable/tutorial/10_key_features/002_configurations.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618da3a6-3097-470a-9c04-fc720c6fa627",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # Define hyperparameter search spaces.\n",
    "    lr = # TODO\n",
    "    momentum = # TODO\n",
    "    kernel_size = # TODO\n",
    "    parameters = {\"lr\": lr, \"momentum\": momentum, \"kernel_size\": kernel_size}\n",
    "    \n",
    "    # Initialize the model with the chosen kernel_size.\n",
    "    net = CNN(kernel_size=kernel_size)\n",
    "    \n",
    "    # Train the model on the training set.\n",
    "    net = train(net, train_loader, parameters)\n",
    "    \n",
    "    # Evaluate validation sets.\n",
    "    val_accuracy = evaluate(net, valid_loader)\n",
    "    \n",
    "    # Return the validation accuracy (this is what we want to maximize).\n",
    "    return val_accuracy\n",
    "\n",
    "# Create Optuna study and optimize.\n",
    "n_trials = 75\n",
    "sampler = optuna.samplers.TPESampler(seed=seed)\n",
    "study = optuna.create_study(sampler=sampler, direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=n_trials)\n",
    "\n",
    "print(\"Best hyperparameters:\", study.best_trial.params)\n",
    "print(\"Best validation accuracy:\", study.best_trial.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df4dd54-78ed-442a-96e9-3a4dde63e6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the optimization history. (Interactive)\n",
    "#fig_history = vis.plot_optimization_history(study)\n",
    "#fig_history.show()\n",
    "\n",
    "# Alternative Visualization. You can use this if the plot is not visible in .pdf version (Matplotlib plot)\n",
    "fig_history = optuna.visualization.matplotlib.plot_optimization_history(study)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aadb4bd-0742-44b7-a106-f2dd53c098fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on the test set using the best hyperparameters.\n",
    "test_accuracy = train_test(study.best_trial.params)\n",
    "print(\"Test accuracy with best hyperparameters:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99882ba4-2e4d-42f6-ba9e-7a9685c9f162",
   "metadata": {},
   "source": [
    "## (1c) Grid search\n",
    "\n",
    "Implement grid search for the same three parameters.\n",
    "\n",
    "Remember that you **need to cover 75 alternatives in total.**\n",
    "\n",
    "So, think about how to best allocate those to cover the three hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55c0c95-9018-48fb-b718-d39ffd35627a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18f94ef-2510-43cd-b207-425ba207428f",
   "metadata": {},
   "source": [
    "# 2. Transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ebe257-d9e9-470c-855a-4423e519b533",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcebf4ae-e0cf-4f99-bda6-12a2c4654d7e",
   "metadata": {},
   "source": [
    "Model - **DO NOT CHANGE THIS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad346a9-e39f-4a55-bae9-4127122091dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNModel, self).__init__()\n",
    "\n",
    "        # Block 1\n",
    "        self.cnn1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=5, stride=1, padding=2)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.MaxPool1 = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        # Block 2\n",
    "        self.cnn2 = nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.MaxPool2 = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        # Block 3\n",
    "        self.cnn3 = nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.MaxPool3 = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        self.fc1 = nn.Linear(576, 10)  # Fully connected layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.MaxPool1(self.bn1(self.relu1(self.cnn1(x))))\n",
    "        out = self.MaxPool2(self.bn2(self.relu2(self.cnn2(out))))\n",
    "        out = self.MaxPool3(self.bn3(self.relu3(self.cnn3(out))))\n",
    "\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc1(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe0911c-7041-4fef-8ad2-1de4b2f8d5fa",
   "metadata": {},
   "source": [
    "Load Fashion-MNIST data. 128 training samples, 10k test samples. **DO NOT CHANGE THIS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c48c70-708c-4542-bb94-51a95ccb34ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "        \n",
    "# Load full dataset\n",
    "full_train_dataset = torchvision.datasets.FashionMNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
    "test_dataset = torchvision.datasets.FashionMNIST(root=\"./data\", train=False, download=True, transform=transform)\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "# Slice small set of data\n",
    "indices = random.sample(range(len(full_train_dataset)), batch_size)\n",
    "train_dataset = Subset(full_train_dataset, indices)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(\"Training set size:\", len(train_loader.dataset))\n",
    "print(\"Test set size:\", len(test_loader.dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3a6d2e-9ead-44b0-9e60-ccd4373cb081",
   "metadata": {},
   "source": [
    "We recommend writing a general training function that implements all three alternative models.\n",
    "\n",
    "Below **train_model()** function is a template that already has some functionality for storing the errors, doing evaluation over test dataset and printing results.\n",
    "\n",
    "You should think about how to implement alternative ways of doing transfer learning.\n",
    "\n",
    "Once you have this function, you can simply run the following cells to try out the alternative approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153e0b55-ed3f-4e10-9f2f-9ae0d567301b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train_model(model, train_loader, test_loader, optimizer, criterion, epochs, freeze_layers):\n",
    "    \n",
    "    if freeze_layers:\n",
    "        \n",
    "        # TODO - Implement what happens when you fix weights of all other layers and train only the last layer.\n",
    "\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    train_accs = []\n",
    "    test_accs = []\n",
    "    \n",
    "    # Training Loop\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss, correct, total = 0, 0, 0\n",
    "        for images, labels in train_loader:\n",
    "            \n",
    "            # TODO - Implement the missing parts of the training loop\n",
    "            \n",
    "            total_loss += loss.item() * images.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "        train_losses.append(total_loss / total)\n",
    "        train_accs.append(100 * correct / total)\n",
    "\n",
    "        # Evaluation\n",
    "        model.eval()\n",
    "        test_loss_total, correct, total = 0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader:\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                test_loss_total += loss.item() * images.size(0)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "        test_losses.append(test_loss_total / total)\n",
    "        test_accs.append(100 * correct / total)\n",
    "\n",
    "        print(f\"Epoch {epoch+1:>2}/{epochs} | \"\n",
    "              f\"Train Loss: {train_losses[-1]:.4f} | Train Acc: {train_accs[-1]:.2f}% | \"\n",
    "              f\"Test Loss: {test_losses[-1]:.4f} | Test Acc: {test_accs[-1]:.2f}%\")\n",
    "\n",
    "    return train_losses, test_losses, train_accs, test_accs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dde5833-9385-406a-97f4-c7e4885d4120",
   "metadata": {},
   "source": [
    "## (2a) From Scratch\n",
    "\n",
    "Create a new CNNModel and train from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e131f0bd-c882-44db-b7f3-b5fb5732bc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = CNNModel()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer1 = torch.optim.Adam(model1.parameters(), lr=0.01)\n",
    "epochs = # TODO\n",
    "freeze_layers = False\n",
    "train_loss1, test_loss1, train_acc1, test_acc1 = train_model(model1, train_loader, test_loader, optimizer1,\n",
    "                                                             criterion, epochs=epochs, freeze_layers=freeze_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651b41ed-fff8-4557-95f4-610f494f01e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_range = range(1, epochs + 1)\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(epoch_range, train_loss1, label=\"Train Loss\")\n",
    "plt.plot(epoch_range, test_loss1, label=\"Test Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss (from scratch)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f376b7b9-cb7e-448f-b424-6965f94fe010",
   "metadata": {},
   "source": [
    "## (2b) Train only last layer of pre-trained model\n",
    "\n",
    "Create a new CNNModel, load pre-trained weights, fix the weights of all other layers and only train the last layer. \n",
    "\n",
    "Remember to implement what happens when **freeze_layers = True** in the given **train_model()** function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3052fc64-1635-48c2-8879-60d7fed3327c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfer Learning: Freeze all but last layer\n",
    "model2 = CNNModel()\n",
    "model2.load_state_dict(torch.load(\"pretrained_MNIST_model.pt\"))\n",
    "optimizer2 = torch.optim.Adam(model2.parameters(), lr=0.01)\n",
    "epochs = # TODO\n",
    "freeze_layers = True\n",
    "train_loss2, test_loss2, train_acc2, test_acc2 = train_model(model2, train_loader, test_loader, optimizer2, \n",
    "                                                             criterion, epochs=epochs, freeze_layers=freeze_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147d8d8b-129e-4b32-97f0-d3af4ac7a0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_range = range(1, epochs + 1)\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(epoch_range, train_loss2, label=\"Train Loss\")\n",
    "plt.plot(epoch_range, test_loss2, label=\"Test Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss (Pre-trained model: train only last layer)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4eb426-9459-42b3-afdd-ee2573d4650b",
   "metadata": {},
   "source": [
    "## (2c) Train all layers of pre-trained model\n",
    "\n",
    "Create a new CNNModel, load pre-trained weights and fine-tune all weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18e992e-1777-45ce-97e4-34ac155ad19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = CNNModel()\n",
    "model3.load_state_dict(torch.load(\"pretrained_MNIST_model.pt\"))\n",
    "optimizer3 = torch.optim.Adam(model3.parameters(), lr=0.01)\n",
    "epochs = # TODO\n",
    "freeze_layers = False\n",
    "train_loss3, test_loss3, train_acc3, test_acc3 = train_model(model3, train_loader, test_loader, optimizer3, \n",
    "                                                             criterion, epochs=epochs, freeze_layers=freeze_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1ff68b-c681-40d9-ac52-f6fe5e441e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_range = range(1, epochs + 1)\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(epoch_range, train_loss3, label=\"Train Loss\")\n",
    "plt.plot(epoch_range, test_loss3, label=\"Test Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss (Pre-trained model: train all layers)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222e98ca-49ca-4cd4-962b-a624da64949a",
   "metadata": {},
   "source": [
    "## (2d) Plot and Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec219bb-1104-4db5-939b-760b3086a638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70226f92-0342-4a55-862f-360d46b48b2e",
   "metadata": {},
   "source": [
    "TODO - Report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4c7e47-d270-4be3-a575-bb85c00d014c",
   "metadata": {},
   "source": [
    "# 3. Few-shot learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0073e3c0-129a-4a70-a50f-be584752dc11",
   "metadata": {},
   "source": [
    "Here **KShotCDataset()** function is given.\n",
    "\n",
    "It is used to create a FashionMNIST dataloader for a given **K** and **C**.\n",
    "\n",
    "**DO NOT CHANGE THIS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b3eae4-e437-467a-8b76-2cc66a4f8d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "class KShotCDataset(Dataset):\n",
    "    def __init__(self, fashion_mnist_dataset, k_shot, c_way):\n",
    "        self.fashion_mnist_dataset = fashion_mnist_dataset\n",
    "        self.k_shot = k_shot\n",
    "        self.c_way = c_way\n",
    "\n",
    "        self.data_indices = []\n",
    "\n",
    "        self.class_indices = {label: [] for label in range(self.c_way)}\n",
    "        self.create_balanced_dataset()\n",
    "\n",
    "    def create_balanced_dataset(self):\n",
    "        for idx, (_, label) in enumerate(self.fashion_mnist_dataset):\n",
    "            if label < self.c_way:\n",
    "                self.class_indices[label].append(idx)\n",
    "\n",
    "        for label in range(self.c_way):\n",
    "            self.data_indices.extend(self.class_indices[label][:self.k_shot])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_indices)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        fashion_mnist_index = self.data_indices[index]\n",
    "        image, label = self.fashion_mnist_dataset[fashion_mnist_index]\n",
    "        return image, label\n",
    "\n",
    "# Load the Fashion MNIST training dataset\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "fashionmnist_dataset = torchvision.datasets.FashionMNIST(\n",
    "    root='./data',\n",
    "    train=True,\n",
    "    transform=transform,\n",
    "    download=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89d9187-d344-4563-8736-b541339117af",
   "metadata": {},
   "source": [
    "Below, you see an example of how to use **KShotCDataset().**\n",
    "\n",
    "Implement the algorithm and follow the exercise sheet for details on what to analyse and report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff005b4-87c9-495a-b088-d93d06cda49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 7  # Number of shots per class\n",
    "C = 10  # Number of classes -- ordered labels are selected, e.g. C = 3 means labels=[0, 1, 2]\n",
    "# Create the K-shot C-way dataset\n",
    "k_shot_c_dataset = KShotCDataset(fashionmnist_dataset, K, C)\n",
    "\n",
    "dataloader = DataLoader(k_shot_c_dataset, batch_size=K*C, shuffle=True)\n",
    "\n",
    "for images, labels in dataloader:\n",
    "    # Use the model for feature extraction ...\n",
    "    # ...\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
