{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "768b51d1",
   "metadata": {},
   "source": [
    "### Below is a concise yet friendly explanation of the attention mechanism for assignment introduction:\n",
    "\n",
    "#### Attention Mechanism (Adapted from ‚ÄúAttention Is All You Need‚Äù)\n",
    "\n",
    "The attention mechanism, introduced in Attention Is All You Need (Vaswani et al., 2017), processes inputs represented as vectors (each row is a token embedding of dimension $ùê∑$). We compute three sets of vectors: queries (Q), keys (K), and values (V). The core steps are:\n",
    "\n",
    "1. **Linear Transformations:**  \n",
    "\n",
    "Let $X$ be the input matrix, where each of the rows corresponds to a token in the input sequence, and each row is a $d$-dimensional embedding vector.\n",
    "\n",
    "To compute attention, we first project $X$ into three different representations using learned weight matrices:\n",
    "\n",
    "Each input vector is transformed into $Q$, $K$, and $V$ using learnable weights.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "Q_i &= X W_i^Q, \\\\\n",
    "K_i &= X W_i^K, \\\\\n",
    "V_i &= X W_i^V.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Each head \\(i\\) has its own learnable parameters $W_i^Q$, $W_i^K$, and $W_i^V$, which transform the input into queries, keys, and values, respectively.\n",
    "\n",
    "\n",
    "2. **Scaled Dot-Product Attention:**  \n",
    "\n",
    "\\begin{equation}\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right)V.\n",
    "\\end{equation}\n",
    "\n",
    "Here, $QK^T$ produces a matrix of scores that measures how relevant each ‚Äúquery‚Äù position is to every ‚Äúkey‚Äù position. $d_k$ is the dimension of queries and keys. \n",
    "\n",
    "The softmax function converts these scores into attention weights (non-negative values that sum to 1 across each row).\n",
    "\n",
    "These weights are then used to combine the values ùëâ to produce the final output.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "3. **Multi-Head Attention:**  \n",
    "\n",
    "$$\n",
    "\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\dots, \\text{head}_h)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{where } \\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V).\n",
    "$$\n",
    "\n",
    "\n",
    "Multiple attention heads allow the model to attend to different aspects of the input simultaneously. For each of these heads we use $d_k = d_{model}/H$. Their outputs are concatenated and linearly transformed to produce the final result.\n",
    "\n",
    "\n",
    "\n",
    "*Note:* In this assignment, you are only required to experiment with the provided $Q$, $K$, and $V$ matrices to perform the matrix multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95ccbf8",
   "metadata": {},
   "source": [
    "## Self-attention Computer Assignment \n",
    "\n",
    "\n",
    "Implement the multi-head self-attention operation, taking in a set of $N$ vectors of $D$ dimensions and outputting a matrix of the same size. Do this without relying on neural network libraries, but rather write directly the required operations in NumPy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434b6c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6598d929-2af6-4d94-acee-1341af2a1e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data size\n",
    "N = 5\n",
    "D = 6\n",
    "\n",
    "X = [[ 0.7, -0.8, -1.2,  -1.,  -0., -0.3],\n",
    "     [ 2.7,  0.1,  1.6,  1.8,  1.5,  0.3],\n",
    "     [ 0.1,  2.6, -0.1, -1.3, -0.5, -0.7],\n",
    "     [ 1.1,  1.5,   1., -0.5,  0.4,  0.4],\n",
    "     [-0.7, -0.7,  0.7, -1.5, -0.8,  1. ]]\n",
    "\n",
    "Wq = [[-1.7,  1.6,  0.9, -0.5,  0.4,  -1.],\n",
    "      [-0.4,  1. , -0.3,  1. ,  0.5,  1.1],\n",
    "      [ 0.4, -0.9,  -1.,  0.5, -1.4,  0. ],\n",
    "      [ 0.3,  1.4, -1.2,  0.2,  0.1,  1.6],\n",
    "      [-0.8,  0.8, -0.7, -1.3,  0.3,  0.8],\n",
    "      [ 1.1,  0.3, -1.5, -2.3,  2.2, -0.7]]\n",
    "\n",
    "Wk = [[ 0.3, -0.4, -1.3,  0.3, -1.7,  1.1],\n",
    "      [-2.3, -1.1,  0.6, -1.2,  2.2,  0.3],\n",
    "      [ 1.1, -0.4, -0.5,  1.9, -1.1, -1.2],\n",
    "      [-0.4,  1. , -1.7,  0. , -3.3, -1.4],\n",
    "      [-0.9, -1.1, -1. ,  1.4,  1.3,  1.2],\n",
    "      [-0.7,  0.4,  0.4, -1.4, -0.2, -0.5]]\n",
    "\n",
    "Wv = [[-0.1,  0.7,  1. , -0.1,  1.6,  0.9],\n",
    "      [ 0.4, -1. , -0.7, -0.6, -0.9, -0.1],\n",
    "      [-0.4,  0.5, -1.4,  0.1,  0.6,  0.4],\n",
    "      [ 1.4, -1.3, -1.3, -0.6,  1.6, -0.2],\n",
    "      [-0.4, -0.6, -1.4, -1. ,  0.4, -0.8],\n",
    "      [ 0.2,  0.5,  0.4, -0.5,  1.4,  2.3]]\n",
    "\n",
    "\n",
    "\n",
    "X = np.array(X)\n",
    "Wq = np.array(Wq)\n",
    "Wk = np.array(Wk)\n",
    "Wv = np.array(Wv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b382714-52d9-4a53-9361-19e81d5b64c6",
   "metadata": {},
   "source": [
    "### (a) Implement the self-attention operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0314ecba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_attention(X, Wq, Wk, Wv):\n",
    "    ...    \n",
    "    return output, attention_weights\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4298990e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the output\n",
    "output, attention_weights = self_attention(X, Wq, Wk, Wv)\n",
    "\n",
    "# Print in a nice format\n",
    "np.set_printoptions(precision=1)\n",
    "print(\"Self-Attention Output:\\n\", output)\n",
    "print(\"Self-Attention Matrix:\\n\", attention_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7644c648-2f0a-4ae2-a50e-17d0c7f412bb",
   "metadata": {},
   "source": [
    "### (b) Implement multi-head attention, using the previously implemented function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e149e4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_head_attention(X, Wq, Wk, Wv, H):\n",
    "    ...\n",
    "    return output, attention_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5563ed82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute multi-head attention\n",
    "H = 3\n",
    "attention_output = multi_head_attention(X, Wq, Wk, Wv, H)\n",
    "# Again print the requested results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17c2abb-23bf-4772-9090-420aafac4639",
   "metadata": {},
   "source": [
    "### (c+d) Provide the answers/explanations requested in the problem sheet:\n",
    "1. Why the results are different?\n",
    "2. What happens if you change the order of two inputs="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7db2be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
