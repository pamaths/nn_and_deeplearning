{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9a3e457",
   "metadata": {},
   "source": [
    "### Below is a concise yet friendly explanation of the attention mechanism for assignment introduction:\n",
    "\n",
    "#### Attention Mechanism (Adapted from ‚ÄúAttention Is All You Need‚Äù)\n",
    "\n",
    "The attention mechanism, introduced in Attention Is All You Need (Vaswani et al., 2017), processes inputs represented as vectors (each row is a token embedding of dimension $ùê∑$). We compute three sets of vectors: queries (Q), keys (K), and values (V). The core steps are:\n",
    "\n",
    "1. **Linear Transformations:**  \n",
    "\n",
    "Let $X$ be the input matrix, where each of the rows corresponds to a token in the input sequence, and each row is a $d$-dimensional embedding vector.\n",
    "\n",
    "To compute attention, we first project $X$ into three different representations using learned weight matrices:\n",
    "\n",
    "Each input vector is transformed into $Q$, $K$, and $V$ using learnable weights.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "Q_i &= X W_i^Q, \\\\\n",
    "K_i &= X W_i^K, \\\\\n",
    "V_i &= X W_i^V.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Each head \\(i\\) has its own learnable parameters $W_i^Q$, $W_i^K$, and $W_i^V$, which transform the input into queries, keys, and values, respectively.\n",
    "\n",
    "\n",
    "2. **Scaled Dot-Product Attention:**  \n",
    "\n",
    "\\begin{equation}\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right)V.\n",
    "\\end{equation}\n",
    "\n",
    "Here, $QK^T$ produces a matrix of scores that measures how relevant each ‚Äúquery‚Äù position is to every ‚Äúkey‚Äù position. \n",
    "\n",
    "The softmax function converts these scores into attention weights (non-negative values that sum to 1 across each row).\n",
    "\n",
    "These weights are then used to combine the values ùëâ to produce the final output.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "3. **Multi-Head Attention:**  \n",
    "\n",
    "$$\n",
    "\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\dots, \\text{head}_h)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{where } \\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V).\n",
    "$$\n",
    "\n",
    "\n",
    "Multiple attention heads allow the model to attend to different aspects of the input simultaneously. Their outputs are concatenated and linearly transformed to produce the final result.\n",
    "\n",
    "\n",
    "\n",
    "*Note:* In this assignment, you are only required to experiment with the provided $Q$, $K$, and $V$ matrices to perform the matrix multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d953bd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6598d929-2af6-4d94-acee-1341af2a1e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data size\n",
    "N = 5\n",
    "D = 6\n",
    "\n",
    "X = [[ 0.7, -0.8, -1.2,  -1.,  -0., -0.3],\n",
    "     [ 2.7,  0.1,  1.6,  1.8,  1.5,  0.3],\n",
    "     [ 0.1,  2.6, -0.1, -1.3, -0.5, -0.7],\n",
    "     [ 1.1,  1.5,   1., -0.5,  0.4,  0.4],\n",
    "     [-0.7, -0.7,  0.7, -1.5, -0.8,  1. ]]\n",
    "\n",
    "Wq = [[-1.7,  1.6,  0.9, -0.5,  0.4,  -1.],\n",
    "      [-0.4,  1. , -0.3,  1. ,  0.5,  1.1],\n",
    "      [ 0.4, -0.9,  -1.,  0.5, -1.4,  0. ],\n",
    "      [ 0.3,  1.4, -1.2,  0.2,  0.1,  1.6],\n",
    "      [-0.8,  0.8, -0.7, -1.3,  0.3,  0.8],\n",
    "      [ 1.1,  0.3, -1.5, -2.3,  2.2, -0.7]]\n",
    "\n",
    "Wk = [[ 0.3, -0.4, -1.3,  0.3, -1.7,  1.1],\n",
    "      [-2.3, -1.1,  0.6, -1.2,  2.2,  0.3],\n",
    "      [ 1.1, -0.4, -0.5,  1.9, -1.1, -1.2],\n",
    "      [-0.4,  1. , -1.7,  0. , -3.3, -1.4],\n",
    "      [-0.9, -1.1, -1. ,  1.4,  1.3,  1.2],\n",
    "      [-0.7,  0.4,  0.4, -1.4, -0.2, -0.5]]\n",
    "\n",
    "Wv = [[-0.1,  0.7,  1. , -0.1,  1.6,  0.9],\n",
    "      [ 0.4, -1. , -0.7, -0.6, -0.9, -0.1],\n",
    "      [-0.4,  0.5, -1.4,  0.1,  0.6,  0.4],\n",
    "      [ 1.4, -1.3, -1.3, -0.6,  1.6, -0.2],\n",
    "      [-0.4, -0.6, -1.4, -1. ,  0.4, -0.8],\n",
    "      [ 0.2,  0.5,  0.4, -0.5,  1.4,  2.3]]\n",
    "X = np.array(X)\n",
    "Wq = np.array(Wq)\n",
    "Wk = np.array(Wk)\n",
    "Wv = np.array(Wv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b382714-52d9-4a53-9361-19e81d5b64c6",
   "metadata": {},
   "source": [
    "### (a) Implement the self-attention operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3485bb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x, axis=-1):\n",
    "    \"\"\"\n",
    "    Stable softmax function with numerical stability optimization\n",
    "    Args:\n",
    "        x: Input array (numpy ndarray)\n",
    "        axis: Axis along which to compute softmax (default: -1)\n",
    "    Returns:\n",
    "        Probability distribution array along specified axis\n",
    "    \"\"\"\n",
    "    max_scores = np.max(x, axis=-1, keepdims=True)\n",
    "    exp_scores = np.exp(x - max_scores)\n",
    "    sum_exp = np.sum(exp_scores, axis=-1, keepdims=True)\n",
    "    \n",
    "    return exp_scores / sum_exp\n",
    "\n",
    "def self_attention(X, Wq, Wk, Wv):\n",
    "    \"\"\"\n",
    "    Transformer self-attention mechanism implementation\n",
    "    Args:\n",
    "        X: Input matrix [num_samples, dim_input]\n",
    "        Wq: Query weight matrix [dim_input, d_k]\n",
    "        Wk: Key weight matrix [dim_input, d_k]\n",
    "        Wv: Value weight matrix [dim_input, d_k]\n",
    "    Returns:\n",
    "        output: Context vectors [num_samples, d_k]\n",
    "        attn_weights: Attention probabilities [num_samples, num_samples]\n",
    "    \"\"\"\n",
    "    \n",
    "    num_samples, dim_input = X.shape\n",
    "    d_k = Wq.shape[-1]\n",
    "    \n",
    "    # Phase 1: Linear Transformations\n",
    "    Q = X @ Wq # [num_samples, d_k]\n",
    "    K = X @ Wk # [num_samples, d_k]\n",
    "    V = X @ Wv # [num_samples, d_k]\n",
    "    \n",
    "    # Phase 2: Scaled Dot-Product Attention\n",
    "    # Compute attention scores and weights\n",
    "    d_k = Q.shape[-1]\n",
    "    scores = (Q @ K.T) / np.sqrt(d_k) # [num_samples, num_samples]\n",
    "    attn_weights = softmax(scores, axis=-1)\n",
    "    \n",
    "    # Phase 3: Output calculation \n",
    "    # Weighted sum of value vectors\n",
    "    attn_weights = softmax(scores, axis=-1)\n",
    "    output = attn_weights @ V # [num_samples, d_k]\n",
    "    return output, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57c003f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Self-Attention Output:\n",
      " [[-0.7 -0.9  0.5  0.1 -5.5 -1.1]\n",
      " [-0.4  0.6  0.6 -0.6  2.   0.5]\n",
      " [ 0.3 -0.1 -2.4 -1.9  4.9  1.8]\n",
      " [-0.7 -0.7  0.4 -0.1 -4.5 -0.7]\n",
      " [-2.   3.3  2.1  1.6 -1.3  2.8]]\n",
      "Self-Attention Weights:\n",
      " [[4.9e-07 4.0e-14 9.9e-01 1.7e-05 6.1e-03]\n",
      " [4.8e-01 3.6e-01 1.5e-01 2.1e-02 4.0e-04]\n",
      " [1.9e-02 5.8e-01 9.4e-02 2.8e-01 3.2e-02]\n",
      " [3.0e-02 1.2e-02 8.5e-01 9.9e-02 1.1e-02]\n",
      " [4.7e-04 7.3e-03 1.6e-02 4.0e-02 9.4e-01]]\n"
     ]
    }
   ],
   "source": [
    "# Compute the output\n",
    "output, attention_weights = self_attention(X, Wq, Wk, Wv)\n",
    "\n",
    "# Print in a nice format\n",
    "np.set_printoptions(precision=1)\n",
    "print(\"Self-Attention Output:\\n\", output)\n",
    "print(\"Self-Attention Weights:\\n\", attention_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7644c648-2f0a-4ae2-a50e-17d0c7f412bb",
   "metadata": {},
   "source": [
    "### (b) Implement multi-head attention, using the previously implemented function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e149e4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_head_attention(X, Wq, Wk, Wv, H):\n",
    "    \"\"\"\n",
    "    Implements multi-head attention mechanism in transformer\n",
    "    Args:\n",
    "        X: Input array [num_samples, dim_input]\n",
    "        Wq: Query weight array [dim_input, dim_input] (concatenated heads)\n",
    "        Wk: Key weight array [dim_input, dim_input]\n",
    "        Wv: Value weight array [dim_input, dim_input]\n",
    "        H: Number of attention heads\n",
    "    Returns:\n",
    "        concatenated: Combined output array [num_samples, dim_input]\n",
    "        all_attn_weights: List of attention matrices per head\n",
    "    \"\"\"\n",
    "    \n",
    "    # Phase 1: Initialization\n",
    "    num_samples, dim_input = X.shape\n",
    "    dim_per_head = dim_input // H # Features per attention head\n",
    "    outputs = []\n",
    "    all_attn_weights = []\n",
    "    \n",
    "    # Phase 2: Head Processing Loop\n",
    "    for h in range(H):\n",
    "        # Calculate slice indices for current head\n",
    "        start = h * dim_per_head\n",
    "        end = start + dim_per_head\n",
    "        \n",
    "        # Slice columns for each head's weights\n",
    "        Wq_h = Wq[:, start:end]  # [dim_input, dim_per_head]\n",
    "        Wk_h = Wk[:, start:end]\n",
    "        Wv_h = Wv[:, start:end]\n",
    "        \n",
    "        # Process single attention head\n",
    "        # Pass weights without transposing (handled in self_attention)\n",
    "        head_output, attn_weights = self_attention(X, Wq_h, Wk_h, Wv_h) \n",
    "\n",
    "        outputs.append(head_output) # [num_samples, dim_per_head]\n",
    "        all_attn_weights.append(attn_weights) # [num_samples, num_samples]\n",
    "        \n",
    "    # Phase 3: Output Concatenation    \n",
    "    concatenated = np.concatenate(outputs, axis=-1) # [num_samples, dim_input]\n",
    "    \n",
    "    return concatenated, all_attn_weights\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4bc5332b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-Head Attention Output, H=1:\n",
      " [[-0.7 -0.9  0.5  0.1 -5.5 -1.1]\n",
      " [-0.4  0.6  0.6 -0.6  2.   0.5]\n",
      " [ 0.3 -0.1 -2.4 -1.9  4.9  1.8]\n",
      " [-0.7 -0.7  0.4 -0.1 -4.5 -0.7]\n",
      " [-2.   3.3  2.1  1.6 -1.3  2.8]]\n"
     ]
    }
   ],
   "source": [
    "# Compute multi-head attention\n",
    "H = 1\n",
    "attention_output,_ = multi_head_attention(X, Wq, Wk, Wv, H)\n",
    "print(\"Multi-Head Attention Output, H=1:\\n\", attention_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5563ed82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-Head Attention Output, H=3:\n",
      " [[-0.7 -0.9  0.7  0.2 -1.5  2.9]\n",
      " [-1.1  0.9 -3.9 -2.9 -3.  -0.5]\n",
      " [-0.7 -0.9  1.6  1.2  5.8  1.5]\n",
      " [-0.7 -0.7 -3.8 -2.8 -5.1 -0.9]\n",
      " [-0.2  0.3  1.   0.2 -1.3  3. ]]\n"
     ]
    }
   ],
   "source": [
    "# Compute multi-head attention\n",
    "H = 3\n",
    "attention_output,_ = multi_head_attention(X, Wq, Wk, Wv, H)\n",
    "print(\"Multi-Head Attention Output, H=3:\\n\", attention_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bc95df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "51a6bdc5",
   "metadata": {},
   "source": [
    "#### We can confirm attention score correctness through PyTorch's built-in MultiheadAttention module by aligning implementations.\n",
    "Note: This verification section isn't part of formal grading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c29fe6e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H=1 Attention Weights (Head 1):\n",
      " [[[-0.7 -0.9  0.5  0.1 -5.5 -1.1]]\n",
      "\n",
      " [[-0.4  0.6  0.6 -0.6  2.   0.5]]\n",
      "\n",
      " [[ 0.3 -0.1 -2.4 -1.9  4.9  1.8]]\n",
      "\n",
      " [[-0.7 -0.7  0.4 -0.1 -4.5 -0.7]]\n",
      "\n",
      " [[-2.   3.3  2.1  1.6 -1.3  2.8]]]\n",
      "H=3 Attention Weights (Head 3):\n",
      " [[[-0.7 -0.9  0.7  0.2 -1.5  2.9]]\n",
      "\n",
      " [[-1.1  0.9 -3.9 -2.9 -3.  -0.5]]\n",
      "\n",
      " [[-0.7 -0.9  1.6  1.2  5.8  1.5]]\n",
      "\n",
      " [[-0.7 -0.7 -3.8 -2.8 -5.1 -0.9]]\n",
      "\n",
      " [[-0.2  0.3  1.   0.2 -1.3  3. ]]]\n"
     ]
    }
   ],
   "source": [
    "# PyTorch Implementation Comparison\n",
    "# To validate custom multi-head attention implementation \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "X_torch = torch.tensor(X, dtype=torch.float32).unsqueeze(1)\n",
    "Wq_torch = torch.tensor(Wq, dtype=torch.float32)\n",
    "Wk_torch = torch.tensor(Wk, dtype=torch.float32)\n",
    "Wv_torch = torch.tensor(Wv, dtype=torch.float32)\n",
    "\n",
    "\n",
    "# PyTorch comparison for H=1\n",
    "mha_H1 = nn.MultiheadAttention(6, 1, batch_first=False)\n",
    "with torch.no_grad():\n",
    "    mha_H1.in_proj_weight.data = torch.cat([Wq_torch.T, Wk_torch.T, Wv_torch.T], dim=0)\n",
    "    mha_H1.out_proj.weight.data = torch.eye(6)\n",
    "    mha_H1.out_proj.bias.data.zero_()\n",
    "output_torch_H1, _ = mha_H1(X_torch, X_torch, X_torch)\n",
    "output_torch_H1 = output_torch_H1.detach().numpy()\n",
    "\n",
    "print(\"H=1 Attention Weights (Head 1):\\n\", output_torch_H1)\n",
    "\n",
    "\n",
    "# PyTorch comparison for H=3\n",
    "mha_H3 = nn.MultiheadAttention(6, 3, batch_first=False)\n",
    "with torch.no_grad():\n",
    "    mha_H3.in_proj_weight.data = torch.cat([Wq_torch.T, Wk_torch.T, Wv_torch.T], dim=0)\n",
    "    mha_H3.out_proj.weight.data = torch.eye(6)\n",
    "    mha_H3.out_proj.bias.data.zero_()\n",
    "output_torch_H3, _ = mha_H3(X_torch, X_torch, X_torch)\n",
    "output_torch_H3 = output_torch_H3.detach().numpy()\n",
    "\n",
    "print(\"H=3 Attention Weights (Head 3):\\n\", output_torch_H3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357f90ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d17c2abb-23bf-4772-9090-420aafac4639",
   "metadata": {},
   "source": [
    "### (c+d) Provide the answers/explanations requested in the problem sheet:\n",
    "1. Why the results are different?\n",
    "2. What happens if you change the order of two inputs="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d19fc3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H=1 Comparison:\n",
      "Original Order Output :\n",
      " [[-0.7 -0.9  0.5  0.1 -5.5 -1.1]\n",
      " [-0.4  0.6  0.6 -0.6  2.   0.5]\n",
      " [ 0.3 -0.1 -2.4 -1.9  4.9  1.8]\n",
      " [-0.7 -0.7  0.4 -0.1 -4.5 -0.7]\n",
      " [-2.   3.3  2.1  1.6 -1.3  2.8]]\n",
      "Swapped Order Output :\n",
      " [[-0.4  0.6  0.6 -0.6  2.   0.5]\n",
      " [-0.7 -0.9  0.5  0.1 -5.5 -1.1]\n",
      " [ 0.3 -0.1 -2.4 -1.9  4.9  1.8]\n",
      " [-0.7 -0.7  0.4 -0.1 -4.5 -0.7]\n",
      " [-2.   3.3  2.1  1.6 -1.3  2.8]]\n"
     ]
    }
   ],
   "source": [
    "# Swap first two input sequences (rows 0 and 1)\n",
    "X_swapped = X.copy() \n",
    "X_swapped[[0,1],:] = X_swapped[[1,0],:]\n",
    "\n",
    "output_original_H1, _ = multi_head_attention(X, Wq, Wk, Wv, H=1)\n",
    "\n",
    "# Swapped order output\n",
    "output_swapped_H1, _ = multi_head_attention(X_swapped, Wq, Wk, Wv, H=1)\n",
    "\n",
    "print(\"H=1 Comparison:\")\n",
    "print(\"Original Order Output :\\n\", output_original_H1)\n",
    "print(\"Swapped Order Output :\\n\", output_swapped_H1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1fed7624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H=3 Comparison:\n",
      "Original Order Output :\n",
      " [[-0.7 -0.9  0.7  0.2 -1.5  2.9]\n",
      " [-1.1  0.9 -3.9 -2.9 -3.  -0.5]\n",
      " [-0.7 -0.9  1.6  1.2  5.8  1.5]\n",
      " [-0.7 -0.7 -3.8 -2.8 -5.1 -0.9]\n",
      " [-0.2  0.3  1.   0.2 -1.3  3. ]]\n",
      "Swapped Order Output :\n",
      " [[-1.1  0.9 -3.9 -2.9 -3.  -0.5]\n",
      " [-0.7 -0.9  0.7  0.2 -1.5  2.9]\n",
      " [-0.7 -0.9  1.6  1.2  5.8  1.5]\n",
      " [-0.7 -0.7 -3.8 -2.8 -5.1 -0.9]\n",
      " [-0.2  0.3  1.   0.2 -1.3  3. ]]\n"
     ]
    }
   ],
   "source": [
    "output_original_H3, _ = multi_head_attention(X, Wq, Wk, Wv, H=3)\n",
    "\n",
    "# Swapped order output\n",
    "output_swapped_H3, _ = multi_head_attention(X_swapped, Wq, Wk, Wv, H=3)\n",
    "\n",
    "print(\"H=3 Comparison:\")\n",
    "print(\"Original Order Output :\\n\", output_original_H3)\n",
    "print(\"Swapped Order Output :\\n\", output_swapped_H3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfd9d64",
   "metadata": {},
   "source": [
    "1. Each head learns to represent different relationships in the data: the attention weights are computed separately and the outputs are concatenated then pulled back."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2180cf6a",
   "metadata": {},
   "source": [
    "2. Swapping the first two inputs only swaps their positions in the output: for attention output matrices, rows 1-2 swapped compared to original The attention weights adjust to the new input order, reflecting permutation equivariance. The output differences will be non-zero but structured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd4f83c-75ba-483f-8c07-55e3cd8598e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
